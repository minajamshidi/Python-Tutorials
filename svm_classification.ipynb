{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Using SVM\n",
    "\n",
    "In this tutorial we use scikitlearn toolbox for suppoort vector machine (SVM) classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the data and split it to train and test sets. Then scale the train data and apply the scaling transformation to test data using train_test_split function of sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd = pd.read_csv('Phenotypic_V1_0b_preprocessed1_extended.csv')\n",
    "x = data_pd.iloc[:, 108:206].to_numpy()\n",
    "y = data_pd.loc[:, ['SEX']].to_numpy()\n",
    "labels = np.unique(y)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, test_size=0.2, random_state=150)\n",
    "\n",
    "# scale the features: we fit the scaler to train data and apply the same transfrom to test data\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the cross-validation (CV) method we use here is called HoldOut, which means we split data to train and test and then train the model of training data and evaluate the performance of the classification on test data. Another cross-validation method is Kfold, in which data is randomely partitioned to K parts and in each run one of the parts is used as test set and the other K-1 parts are used as training set. The advantage of Kfold over HoldOut is that in Kfold the model sees all the data points. We use Kfold for parameter selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train a kernel SVM classifier. For that we need to find the best parameters based on the training data. For this, we do a grid search in the space of the parameters. For each point of the grid, we calculate the classifier's performance in CV. The parameter set with the best performance is selected as the final parameter set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the data exploration part, the classes are highly unbalanced. Therefore, if the classifier treats the two classes in the same way, it will classify all he data points as class 1. However, sklearn has the option class_weight, in which one can specify the weights of the classes. You can try different values for cw in he following code, or set it as a parameter in grid search and select the best class-weight parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'C': 0.001, 'class_weight': {1: 0.2, 2: 0.8}, 'gamma': 0.001} with a score of 0.85\n"
     ]
    }
   ],
   "source": [
    "# select parameters\n",
    "C_range = np.logspace(-3, 3, 13)\n",
    "gamma_range = np.logspace(-3, 3, 13)\n",
    "#weights = np.linspace(0.03, 0.97, 55)\n",
    "cw = [{1: 0.2, 2:0.8}]#[{1: x, 2:1-x} for x in weights]\n",
    "param_grid = dict(gamma=gamma_range, C=C_range, class_weight=cw) # , class_weight=cw\n",
    "\n",
    "cv = StratifiedShuffleSplit(n_splits=5, train_size=0.8, test_size=0.2)\n",
    "grid = GridSearchCV(SVC(kernel='rbf'), param_grid=param_grid, cv=cv, n_jobs=2)\n",
    "grid.fit(x_train, y_train[:, 0])\n",
    "print(\"The best parameters are %s with a score of %0.2f\"\n",
    "      % (grid.best_params_, grid.best_score_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the best paramenters, we train the classifier on the whole dataset and then apply it on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy=0.8502\n",
      "confusion matrix=\n",
      "[[176   0]\n",
      " [ 31   0]]\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(C=grid.best_params_['C'], gamma=grid.best_params_['gamma'], class_weight={1: 0.2, 2:0.8})\n",
    "clf.fit(x_train, y_train[:, 0])\n",
    "y_test_pred_1 = clf.predict(x_test)\n",
    "acc_1 = np.mean(y_test_pred_1 == y_test[:,0])\n",
    "print('accuracy=%.4f'%(acc_1))\n",
    "conf_mat = confusion_matrix(y_test, y_test_pred_1)\n",
    "print('confusion matrix=\\n%s' %(conf_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the classifier does not have a good performance!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
